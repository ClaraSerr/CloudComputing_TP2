chmod 400 /home/claraserr/Documents/LOG8415E/assignment2_key.pem
ssh -i assignment2_key.pem azureuser@20.220.231.74

sudo addgroup hadoop_
sudo adduser --ingroup hadoop_ hduser_
sudo adduser hduser_ sudo
su hduser_

ssh-keygen -t rsa -P ""
cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

ssh localhost

sudo apt-get update
sudo apt-get install default-jdk

wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz

sudo tar -xzvf hadoop-3.3.4.tar.gz

sudo mv hadoop-3.3.4 hadoop
sudo chown -R hduser_:hadoop_ hadoop

echo "#Set HADOOP_HOME
export HADOOP_HOME=~/hadoop
#Set JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
# Add bin/ directory of Hadoop to PATH
export PATH=$PATH:$HADOOP_HOME/bin" >> .bashrc

. ~/.bashrc

cd /hadoop/etc/hadoop

sed -i 's/# export JAVA_HOME=/export JAVA_HOME=\/usr\/lib\/jvm\/java-11-openjdk-amd64/' hadoop-env.sh

sed -i 's/<\/configuration>/<property>\n<name>hadoop.tmp.dir<\/name>\n<value>\/app\/hadoop\/tmp<\/value>\n<description>Parent directory for other temporary directories.<\/description>\n<\/property>\n<property><name>fs.defaultFS <\/name>\n<value>hdfs:\/\/localhost:54310<\/value>\n<description>The name of the default file system. <\/description>\n<\/property>\n<\/configuration>/' core-site.xml

sudo mkdir -p /app/hadoop/tmp
sudo chown -R hduser_:hadoop_ /app/hadoop/tmp
sudo chmod 750 /app/hadoop/tmp

sudo su
echo "export HADOOP_HOME=~/hadoop" > /etc/profile.d/hadoop.sh
echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> /etc/profile.d/hadoop.sh

exit

cd /hadoop/etc/hadoop
sed -i 's/<\/configuration>/<property>\n<name>mapreduce.jobtracker.address<\/name>\n<value>localhost:54311<\/value>\n<description>MapReduce job tracker runs at this host and port.\n<\/description>\n<\/property>\n<\/configuration>/' mapred-site.xml

sed -i 's/<\/configuration>/<property>\n<name>dfs.replication<\/name>\n<value>1<\/value>\n<description>Default block replication.<\/description>\n<\/property>\n<property>\n<name>dfs.datanode.data.dir<\/name>\n<value>\/home\/hduser_\/hdfs<\/value>\n<\/property>\n<\/configuration>/' hdfs-site.xml

sudo mkdir -p /home/hduser_/hdfs
sudo chown -R hduser_:hadoop_ /home/hduser_/hdfs
sudo chmod 750 /home/hduser_/hdfs

$HADOOP_HOME/bin/hdfs namenode -format

$HADOOP_HOME/sbin/start-dfs.sh

$HADOOP_HOME/sbin/start-yarn.sh


curl  http://www.gutenberg.org/cache/epub/4300/pg4300.txt -o input/pg4300.txt



$HADOOP_HOME/bin/hdfs dfs -mkdir /user
hduser_@assignment2:~$ $HADOOP_HOME/bin/hdfs dfs -mkdir /user/admin
hduser_@assignment2:~$ $HADOOP_HOME/bin/hdfs dfs -mkdir input
mkdir: `hdfs://localhost:54310/user/hduser_': No such file or directory
hduser_@assignment2:~$ ls
hadoop  hadoop-3.3.4.tar.gz  hdfs  input
hduser_@assignment2:~$ $HADOOP_HOME/bin/hdfs dfs -mkdir /user/hduser_
hduser_@assignment2:~$ $HADOOP_HOME/bin/hdfs dfs -mkdir input

scp -i assignment2_key.pem /home/claraserr/Documents/LOG8415E/CloudComputing_TP2/pg4300.txt azureuser@20.220.231.74:~/

sudo cp /home/azureuser/pg4300.txt /home/hduser_/pg4300.txt && sudo chown hduser_:hadoop_ /home/hduser_/pg4300.txt

$HADOOP_HOME/bin/hdfs dfs -copyFromLocal pg4300.txt input


$HADOOP_HOME/bin/hadoop jar hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar wordcount input/pg4300.txt output

$HADOOP_HOME/bin/hdfs dfs -rm output/*
$HADOOP_HOME/bin/hdfs dfs -rm -r output
time $HADOOP_HOME/bin/hadoop jar hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar wordcount input4300/pg4300.txt output

real	0m6.885s
user	0m11.141s
sys	0m0.454s


cat pg4300.txt | tr ' ' '\n' | sort | uniq -c
time cat pg4300.txt | tr ' ' '\n' | sort | uniq -c

real	0m0.697s
user	0m0.492s
sys	0m0.212s


sudo apt-get install python3-pip
pip install pyspark

$HADOOP_HOME/bin/hdfs dfs -rm input/pg4300.txt 
$HADOOP_HOME/bin/hdfs dfs -rm output/*
$HADOOP_HOME/bin/hdfs dfs -rm -r output
wget -P input https://tinyurl.com/4vxdw3pa
wget -P input https://tinyurl.com/kh9excea
wget -P input https://tinyurl.com/dybs9bnk
wget -P input https://tinyurl.com/datumz6m
wget -P input https://tinyurl.com/j4j4xdw6
wget -P input https://tinyurl.com/ym8s5fm4
wget -P input https://tinyurl.com/2h6a75nk
wget -P input https://tinyurl.com/vwvram8
wget -P input https://tinyurl.com/weh83uyn
$HADOOP_HOME/bin/hdfs dfs -copyFromLocal input/* input


3 times :

time $HADOOP_HOME/bin/hadoop jar hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar wordcount input output
$HADOOP_HOME/bin/hdfs dfs -rm output/*
$HADOOP_HOME/bin/hdfs dfs -rm -r output

1st time :
real	0m8.180s
user	0m13.139s
sys	0m0.581s

2nd time :
real	0m8.031s
user	0m13.146s
sys	0m0.544s

3rd time :
real	0m8.204s
user	0m13.575s
sys	0m0.675s


pip3 install pyspark

echo "import os
import pyspark
from pyspark import SparkContext, SparkConf

 
 
if __name__ == '__main__':
    directory = 'input'

    sc = SparkContext('local','PySpark Word Count Exmaple')
    RDD = sc.emptyRDD()
    for filename in os.listdir(directory):
        f = os.path.join(directory, filename)
    
        if os.path.isfile(f):
            words = sc.textFile(f).flatMap(lambda line: line.split(' '))
            wordMap = words.map(lambda word: (word, 1))
            wordCounts = wordMap.reduceByKey(lambda a,b:a +b)
        RDD=RDD.union(wordCounts)
    RDD = RDD.reduceByKey(lambda x,y : x+y)
# save the counts to output
    RDD.saveAsTextFile(os.getcwd()+'/pysparkOutput')" | sudo tee app.py

time python3 app.py

real	0m12.297s
user	0m0.261s
sys	0m0.051s


