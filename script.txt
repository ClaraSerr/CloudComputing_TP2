# Download the instance key
# protect the key against accidental overwriting
chmod 400 /home/claraserr/Documents/LOG8415E/assignment2_key.pem

# Connect to instance with Public IP address
ssh -i assignment2_key.pem azureuser@20.220.228.6

# Add user for hadoop setup
# Password set as admin
sudo addgroup hadoop_
sudo adduser --ingroup hadoop_ hduser_

# Give sudo right to hduser_
sudo adduser hduser_ sudo

# Login as huduser_
su hduser_

# Set up ssh for hadoop
# Create new key
ssh-keygen -t rsa -P ""

# Enable ssh access
cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

# Test connection
ssh localhost

# Install Java
admin | sudo apt-get update
y | sudo apt-get install default-jdk

# Install Hadoop
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz
sudo tar -xzvf hadoop-3.3.4.tar.gz

sudo mv hadoop-3.3.4 hadoop
sudo chown -R hduser_:hadoop_ hadoop

# Add paths to bashrc
echo "#Set HADOOP_HOME
export HADOOP_HOME=~/hadoop
#Set JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
# Add bin/ directory of Hadoop to PATH
export PATH=$PATH:$HADOOP_HOME/bin" >> .bashrc

. ~/.bashrc


# Update configuration files
cd /hadoop/etc/hadoop

sed -i 's/# export JAVA_HOME=/export JAVA_HOME=\/usr\/lib\/jvm\/java-11-openjdk-amd64/' hadoop-env.sh

sed -i 's/<\/configuration>/<property>\n<name>hadoop.tmp.dir<\/name>\n<value>\/app\/hadoop\/tmp<\/value>\n<description>Parent directory for other temporary directories.<\/description>\n<\/property>\n<property><name>fs.defaultFS <\/name>\n<value>hdfs:\/\/localhost:54310<\/value>\n<description>The name of the default file system. <\/description>\n<\/property>\n<\/configuration>/' core-site.xml

sudo mkdir -p /app/hadoop/tmp
sudo chown -R hduser_:hadoop_ /app/hadoop/tmp
sudo chmod 750 /app/hadoop/tmp

sudo su
echo "export HADOOP_HOME=~/hadoop" > /etc/profile.d/hadoop.sh
echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> /etc/profile.d/hadoop.sh

exit

cd /hadoop/etc/hadoop
sed -i 's/<\/configuration>/<property>\n<name>mapreduce.jobtracker.address<\/name>\n<value>localhost:54311<\/value>\n<description>MapReduce job tracker runs at this host and port.\n<\/description>\n<\/property>\n<\/configuration>/' mapred-site.xml

sed -i 's/<\/configuration>/<property>\n<name>dfs.replication<\/name>\n<value>1<\/value>\n<description>Default block replication.<\/description>\n<\/property>\n<property>\n<name>dfs.datanode.data.dir<\/name>\n<value>\/home\/hduser_\/hdfs<\/value>\n<\/property>\n<\/configuration>/' hdfs-site.xml

sudo mkdir -p /home/hduser_/hdfs
sudo chown -R hduser_:hadoop_ /home/hduser_/hdfs
sudo chmod 750 /home/hduser_/hdfs

$HADOOP_HOME/bin/hdfs namenode -format

# Start single node cluster
$HADOOP_HOME/sbin/start-dfs.sh

$HADOOP_HOME/sbin/start-yarn.sh

jps

# Make the HDFS directories required to execute MapReduce jobs

$HADOOP_HOME/bin/hdfs dfs -mkdir /user

$HADOOP_HOME/bin/hdfs dfs -mkdir /user/hduser_
$HADOOP_HOME/bin/hdfs dfs -mkdir input4300

# Copy pg4300.txt from local computer to VM

scp -i assignment2_key.pem /home/claraserr/Documents/LOG8415E/CloudComputing_TP2/pg4300.txt azureuser@20.220.228.6:~/

# Copy from user azureuser to user hduser_
sudo cp /home/azureuser/pg4300.txt /home/hduser_/pg4300.txt && sudo chown hduser_:hadoop_ /home/hduser_/pg4300.txt

# Put file into hdfs
$HADOOP_HOME/bin/hdfs dfs -copyFromLocal pg4300.txt input4300

# Run wordcount
$HADOOP_HOME/bin/hadoop jar hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar wordcount input4300/pg4300.txt output

# Delete output folder and its content to run jobs again
$HADOOP_HOME/bin/hdfs dfs -rm output/*
$HADOOP_HOME/bin/hdfs dfs -rm -r output

# Performance measure
time $HADOOP_HOME/bin/hadoop jar hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar wordcount input4300/pg4300.txt output

$HADOOP_HOME/bin/hdfs dfs -rm output/*
$HADOOP_HOME/bin/hdfs dfs -rm -r output

# OUTPUT
#real	0m6.885s
#user	0m11.141s
#sys	0m0.454s

# Linux wordcount
cat pg4300.txt | tr ' ' '\n' | sort | uniq -c
time cat pg4300.txt | tr ' ' '\n' | sort | uniq -c

# OUTPUT
#real	0m0.697s
#user	0m0.492s
#sys	0m0.212s


# Instal python and pyspark

sudo apt-get install python3-pip
pip3 install pyspark

# Install Spark 

wget https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz 

sudo tar -xzvf spark-3.3.1-bin-hadoop3.tgz 
sudo mv spark-3.3.1-bin-hadoop3 /usr/local/spark
echo "export PATH=$PATH:/usr/local/spark/bin" >> .bashrc
source ~/.bashrc

# Download dataset

wget -P input https://tinyurl.com/4vxdw3pa
wget -P input https://tinyurl.com/kh9excea
wget -P input https://tinyurl.com/dybs9bnk
wget -P input https://tinyurl.com/datumz6m
wget -P input https://tinyurl.com/j4j4xdw6
wget -P input https://tinyurl.com/ym8s5fm4
wget -P input https://tinyurl.com/2h6a75nk
wget -P input https://tinyurl.com/vwvram8
wget -P input https://tinyurl.com/weh83uyn

$HADOOP_HOME/bin/hdfs dfs -mkdir input

$HADOOP_HOME/bin/hdfs dfs -copyFromLocal input/* input




